{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №6. Ансамбли: случайный лес, градиентный бустинг.\n",
    "\n",
    "## Ф.И.О: _________\n",
    "\n",
    "### Описание.\n",
    "\n",
    "Домашнее задание состоит из 2-х частей:\n",
    "  - реализация модулей:\n",
    "    - bagger\n",
    "    - booster\n",
    "    - sampler\n",
    "-------------------\n",
    " * Реализации можно частично проверить через юнит-тесты (запускаются командой ``pytest tests``).\n",
    "-------------------\n",
    "  - экспериментальная часть\n",
    "\n",
    "На проверку требуется отправить zip архив, который будет содержать следующие файлы:\n",
    "  - модуль ``ensemble`` с реализованными модулями(bagger, booster, sampler)\n",
    "  - заполненный блокнот (эксперименты) в формате .ipynb \n",
    "  - заполненный блокнот в формате .html (File -> Save and Export Notebook As -> HTML -> ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Сэмплирование случайных объектов и признаков (2 points)\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "`BaseSampler` класс, который будет упрощать семплирование различных подмассивов данных. \n",
    "Реализуйте метод:\n",
    "  - `sample_indices`, который по числу сущностей `n_objects` возращает случайную подвыборку индексов.\n",
    "\n",
    "Используйте атрибуты:\n",
    "  - `self.random_state`, чтобы результаты семпплирования воспроиводились\n",
    "  - `self.bootstrap`, если нужно выбрать случайную подвыборку с возвращением.\n",
    "\n",
    "У класса `ObjectSampler` реализован метод:\n",
    "  - `sample`, который возвращает случайную подвыборку объектов обучения и ответы для них.\n",
    "\n",
    "В классе `FeaturesSampler` реализован метод:\n",
    "- `sample`, который возвращает случайную подвыборку признаков для объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Бэггинг (5 points)\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких \"слабых\" базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Вам предлагается реализовать несколько методов класса `Bagger`:\n",
    "* `fit` - обучение базовых моделей\n",
    "* `predict_proba` - вычисление вероятностей ответов.\n",
    "\n",
    "В данном задании реализация **случайного леса будет бэггингом над решающими деревьями**. \n",
    "\n",
    "Реализация случайного леса представлена в классе `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Градиентный бустинг (7 points)\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. \n",
    "\n",
    "Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. У каждого следующего алгоритма вычисляется ___вес___ $\\omega$, с которым он входит в ансамбль. \n",
    "\n",
    "Также есть параметр скорости обучения (___learning rate___ - $lr$), который не позволяет алгоритму переобучитсья. \n",
    "\n",
    "Вес $\\omega$ можно находить, используя одномерную оптимизацию. \n",
    "\n",
    "Рассмотрим процедуру обучения по шагам (будем рассматривать случай бинарной классификации c метками классов {0,1}, чтобы не усложнять жизнь):\n",
    "1. Настройка базового алгоритма $b_0$ (в данном случае это решающее дерево).\n",
    "    \n",
    "    $$\\text{Алгоритм настраиваются на $y$ с помощью функции MSE.}$$\n",
    "    \n",
    "2. Будем обозначать текущий небазовый алгоритм - $a$:\n",
    "    \n",
    "    $$a_i(x) = \\sum_{j=0}^i \\omega_j b_j(x) $$\n",
    "    \n",
    "3. Настройка базового алгоритма $b_i$ (обычно это регрессионное дерево):\n",
    "    \n",
    "    $$b_i = \\arg \\min_b \\sum_{j=1}^l (b(x_j) + \\nabla L(a_{i-1}(x_j), y))^2,$$\n",
    "    т.е. выход очередного базового алгоритма подстраивается под антиградиент функции потерь\n",
    "    \n",
    "4. Настройка веса базового алгоритма $\\omega_i$:\n",
    "    \n",
    "    $$\\omega_i = \\min_{\\omega > 0} \\sum_{j=1}^l L(a_{i-1} + \\omega b_i(x_j), y) $$\n",
    "    \n",
    "Для задачи классфикации будем использовать логистическую функцию потерь. Немного упростим ее:\n",
    "\n",
    "$$L = -y\\log\\sigma(a) - (1-y)\\log(1 - \\sigma(a)) = -\\log(1 - \\sigma(a)) - y \\log \\frac{\\sigma(a)}{1 - \\sigma(a)},$$\n",
    "где $\\sigma$ - функция сигмоиды. \n",
    "\n",
    "Ответ после очередного базового алгоритма надо прогонять через сигмоиду, т.к. не гарантируется, что ответы будут лежать на [0,1] - в этом особенность базового алгоритма (который является регрессионным).\n",
    "\n",
    "Преобразуем:\n",
    "$$\\log (1 - \\sigma(a)) = \\log \\frac{1}{1 + \\exp(a)} = -\\log(1 + \\exp(a)) $$\n",
    "\n",
    "$$\\log (\\frac{\\sigma(a)}{1 - \\sigma(a)}) = \\log(\\exp(a)) = a $$\n",
    " \n",
    "Таким образом:\n",
    "\n",
    "$$L = -ya + \\log(1 + \\exp(a))$$\n",
    "\n",
    "Тогда будем вычислять градиент как:\n",
    " \n",
    "$$\\nabla L = - y + \\sigma(a)$$\n",
    "\n",
    "В классе `Booster` реализуйте методы:\n",
    "* `_fit_first_estimator` – построение первой модели (первого приближения данных);\n",
    "* `fit` – обучение алгоритма градиентного бустинга (обучение первой и последующих базовых моделей);\n",
    "* `predict` – получение предсказаний алгоритма градиентного бустинга.\n",
    "\n",
    "В классе `GradientBoostingClassifier` реализуйте методы:\n",
    "* `_fit_base_estimator` - обучение очередной базовой модели;\n",
    "* `_gradient` - расчет градиента функции ошибки;\n",
    "* `_loss` - расчет функции ошибки (для одномерно оптимизации)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Эксперименты (6 points)\n",
    "\n",
    "Скачайте датасейт для экспериментов: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
    "\n",
    "Колонка с ответами - RainTommorow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weatherAUS.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделите признаки год/месяц/день:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = # put your code here\n",
    "data['month'] = # put your code here\n",
    "data['day'] = # put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие года есть в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на три части (train, val и test) по временному принципу:\n",
    "    \n",
    "* train - 2007-2014\n",
    "* val - 2015\n",
    "* test - 2016-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = {\n",
    "    'train': # put your code here,\n",
    "    'val': # put your code here,\n",
    "    'test': # put your code here,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вы можете делать всевозможные преобразования признаков. \n",
    "\n",
    "Для того, чтобы получить качество, необходимое для преодоления бейзлайна, вам достаточно закодировать все категориальные признаки с помощью `LabelEncoder`, а также разумно обработать пропущенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш таргет - **RainTommorow**. Удалите его из обучающих данных, также удалите признак RISK_MM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data['RainTomorrow']\n",
    "data.drop(['RainTomorrow', 'RISK_MM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data[indexes['train']].values, target_data[indexes['train']].values\n",
    "X_val, y_val = data[indexes['val']].values, target_data[indexes['val']].values\n",
    "X_test, y_test = data[indexes['test']].values, target_data[indexes['test']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого из алгоритмов достигнутое качество (**accuracy**) должно быть: \n",
    "* ***RandomForest > 0.84 (2 points)***\n",
    "* ***GradientBoosting > 0.845 (2 points)***\n",
    "* ***Отчет об экспериментах (2 points)***\n",
    "\n",
    "Обучите каждый из алгоритмов до нужного качества, используйте валидационную выборку, чтобы подбирать гиперпараметры. Получите качество (accuracy) выше необходимого и на validation, и на test.\n",
    "\n",
    "**Подсказка:** для визуализации прогресса обучения можно использовать бибилиотеку [`tqdm`](https://tqdm.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Бонус. [AdaBoost](https://ru.wikipedia.org/wiki/AdaBoost) (5 points)\n",
    "\n",
    "В алгоритме AdaBoost всем объектам обучения присваивается вес `weight`, который определяет степень важности объекта при обучении. И если текущая модель ошибается на некотором объекте, его вес увеличивается, и этот объект будет больше влиять на обучение следующей модели. \n",
    "\n",
    "Также, так как модели обучаются последовательно, они не равносильны между собой, поэтому у каждой модели тоже есть вес `alpha`, который определяет вес модели при суммировании ответов и зависит от количества ошибок `err` модели. На каждой итерации обучения, эти веса пересчитываются по формулам:\n",
    "\n",
    "$$\\alpha_j = \\log\\left(\\frac{1-err_j}{err_j}\\right),$$\n",
    "где $err_j$ - ошибка классификации\n",
    "\n",
    "$$w_{new}^t = \\frac{w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^t) \\cdot b_j(x^t))}{\\sum\\limits_{i=1}^m w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^i) \\cdot b_j(x^i))}$$\n",
    "\n",
    "Изначально все веса объектов $w^i$ равны (и нормированы на 1).\n",
    "\n",
    "Вам предлагается полностью реализовать AdaBoost. Вы можете использовать предыдущие шаблоны, но учтите некоторые пункты:\n",
    "* надо работать с метками {-1,1} - это обусловлено использованием экспоненциальной функции потерь\n",
    "* метод `predict` представляет собой функцию сигмоид, примененную к сумме предсказаний всех моделей\n",
    "\n",
    "Для реализации AdaBoost на датасете, полученном в предыдущем пункте требуется добиться точноcти:\n",
    "* AdaBoost > 0.83\n",
    "\n",
    "Полезные ссылки:\n",
    "\n",
    "- [статья](https://face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf)\n",
    "- [nerc](https://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3,_AdaBoost)\n",
    "- [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
